// ====== í™˜ê²½ ì„¤ì • (API í‚¤: ì‹¤ì œ í‚¤ë¡œ êµì²´í•˜ì„¸ìš”. ë³´ì•ˆìƒ ì„œë²„ë¡œ ì˜®ê¸°ì„¸ìš”) ======
const TTS_API_KEY = 'HdyjTuGXQOK8xoBfZARtvdaqOeEbZoLLad2FLCKCc2Jx';  // os.getenv('TTS_API_KEY')
const TTS_URL = 'https://api.us-south.text-to-speech.watson.cloud.ibm.com/instances/5421e28b-1e74-4f93-9c05-808dcd0b7ca7';  // os.getenv('TTS_URL')
const STT_API_KEY = 'lkDU8YLMlsM151S0B4gPGoYOg10KMm1GVeQl9u_zKqO0';  // os.getenv('STT_API_KEY')
const STT_URL = 'https://api.us-south.speech-to-text.watson.cloud.ibm.com/instances/2a088970-da1a-4ff1-b35e-988423300d08';  // os.getenv('STT_URL')
const IBM_URL = 'https://us-south.ml.cloud.ibm.com/';  // os.getenv('IBM_URL')
const PROJECT_ID = 'ca4ab8b0-0c39-4985-9400-2df348bbf894';  // os.getenv('PROJECT_ID')
const API_KEY = 'Q07iDID0ogWVEBA_cDtcEshvkcyqwF5HIzV75YOID-K8';  // os.getenv('API_KEY')

// ====== ìƒìˆ˜ (Pythonê³¼ ìœ ì‚¬) ======
const SAMPLE_RATE = 16000;  // RATE
const CHUNK_SIZE = 1024;    // CHUNK
const SILENCE_THRESHOLD = 0.1;  // Pythonì˜ 1000ì„ normalized (0-1 ë²”ìœ„)ë¡œ ì¡°ì •
const SILENCE_DURATION = 1000;   // 1ì´ˆ (ms)
const MAX_RECORD_SECONDS = 10;   // RECORD_SECONDS

let isVoiceMode = false;
let mediaRecorder;
let audioContext;
let analyser;
let stream;
let silenceStart;
let isSilent = false;
let audioChunks = [];

// UI ìš”ì†Œ
const toggleButton = document.getElementById('toggleVoice');
const statusDiv = document.getElementById('status');
const responseDiv = document.getElementById('response');
const ttsPlayer = document.getElementById('ttsPlayer');

// ====== ìŒì„± ëª¨ë“œ í† ê¸€ ======
toggleButton.addEventListener('click', async () => {
    if (!isVoiceMode) {
        try {
            await startVoiceMode();
            isVoiceMode = true;
            toggleButton.textContent = 'Stop Voice Mode';
            statusDiv.textContent = 'ìŒì„± ëª¨ë“œ í™œì„±í™”. ë§ì”€í•´ì£¼ì„¸ìš”...';
        } catch (error) {
            console.error('ìŒì„± ëª¨ë“œ ì‹œì‘ ì‹¤íŒ¨:', error);
            statusDiv.textContent = 'ë§ˆì´í¬ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤.';
        }
    } else {
        stopVoiceMode();
        isVoiceMode = false;
        toggleButton.textContent = 'Start Voice Mode';
        statusDiv.textContent = 'ì¤€ë¹„ ì¤‘...';
    }
});

// ====== ìŒì„± ëª¨ë“œ ì‹œì‘ (ë…¹ìŒ + ì¹¨ë¬µ ê°ì§€) ======
async function startVoiceMode() {
    stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    audioContext = new AudioContext();
    analyser = audioContext.createAnalyser();
    const source = audioContext.createMediaStreamSource(stream);
    source.connect(analyser);
    analyser.fftSize = 2048;

    mediaRecorder = new MediaRecorder(stream);
    mediaRecorder.ondataavailable = (event) => audioChunks.push(event.data);
    mediaRecorder.onstop = processRecording;

    audioChunks = [];
    silenceStart = Date.now();
    mediaRecorder.start();
    detectSilence();  // ì¹¨ë¬µ ê°ì§€ ë£¨í”„ ì‹œì‘
}

// ====== ì¹¨ë¬µ ê°ì§€ (Pythonì˜ is_silent ë£¨í”„) ======
function detectSilence() {
    if (!isVoiceMode) return;

    const dataArray = new Float32Array(analyser.frequencyBinCount);
    analyser.getFloatFrequencyData(dataArray);
    const volume = Math.max(...dataArray) / 100 + 1;  // Normalize to 0-1

    if (volume < SILENCE_THRESHOLD) {
        if (!isSilent) {
            silenceStart = Date.now();
            isSilent = true;
        }
        if (Date.now() - silenceStart > SILENCE_DURATION) {
            mediaRecorder.stop();  // ì¹¨ë¬µ ì‹œ ë…¹ìŒ ì¢…ë£Œ
            return;
        }
    } else {
        isSilent = false;
    }

    // ìµœëŒ€ ì‹œê°„ ì´ˆê³¼ ì²´í¬
    if (Date.now() - silenceStart > MAX_RECORD_SECONDS * 1000) {
        mediaRecorder.stop();
        statusDiv.textContent = 'ë…¹ìŒ ì‹œê°„ ì´ˆê³¼';
        return;
    }

    requestAnimationFrame(detectSilence);  // ë£¨í”„
}

// ====== ë…¹ìŒ ì¢…ë£Œ ë° ì²˜ë¦¬ (STT â†’ LLM â†’ TTS) ======
async function processRecording() {
    const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });
    statusDiv.textContent = 'ìŒì„± ì²˜ë¦¬ ì¤‘...';

    try {
        // STT: ìŒì„± â†’ í…ìŠ¤íŠ¸
        const userText = await speechToText(audioBlob);
        if (!userText) throw new Error('ìŒì„± ì¸ì‹ ì‹¤íŒ¨');

        statusDiv.textContent = `ì‚¬ìš©ì: ${userText}`;
        console.log('ğŸ‘¤ ì‚¬ìš©ìì˜ ë§:', userText);

        // LLM: AI ì‘ë‹µ ìƒì„±
        const prompt = userText + " ë‹¹ì‹ ì€ ì˜ì‚¬ì…ë‹ˆë‹¤. í™˜ìì˜ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”.";
        const aiResponse = await getCompletion(prompt);
        responseDiv.textContent = `Dr. Watson: ${aiResponse}`;
        console.log('ğŸ¤– ì‘ë‹µ:', aiResponse);

        // TTS: í…ìŠ¤íŠ¸ â†’ ìŒì„± ì¬ìƒ
        await textToSpeech(aiResponse);
    } catch (error) {
        console.error('ì²˜ë¦¬ ì‹¤íŒ¨:', error);
        statusDiv.textContent = 'ì˜¤ë¥˜ ë°œìƒ: ' + error.message;
    } finally {
        // ìë™ ì¬ë…¹ìŒ (Pythonì˜ while Trueì²˜ëŸ¼)
        if (isVoiceMode) {
            audioChunks = [];
            mediaRecorder.start();
            silenceStart = Date.now();
            detectSilence();
            statusDiv.textContent = 'ë‹¤ì‹œ ë§ì”€í•´ì£¼ì„¸ìš”...';
        }
    }
}

// ====== STT (ìŒì„± â†’ í…ìŠ¤íŠ¸, IBM Watson) ======
async function speechToText(audioBlob) {
    const formData = new FormData();
    formData.append('audio', audioBlob, 'recorded.wav');
    const sttModel = 'ko-KR_BroadbandModel';
    const sttEndpoint = `${STT_URL}/v1/recognize?model=${sttModel}`;

    const response = await fetch(sttEndpoint, {
        method: 'POST',
        headers: { 'Accept': 'application/json' },
        body: formData,
        // auth: fetchëŠ” Basic Auth ì‚¬ìš© (apikey:STT_API_KEY)
        credentials: 'include'  // í•„ìš” ì‹œ
    });

    if (response.ok) {
        const result = await response.json();
        const recognized = result.results?.[0]?.alternatives?.[0]?.transcript;
        return recognized || '';
    }
    throw new Error('STT ì‹¤íŒ¨: ' + response.status);
}

// ====== LLM (IBM Watson ML API í˜¸ì¶œ) ======
async function getCompletion(prompt) {
    const response = await fetch(`${IBM_URL}/ml/v1/text/generation?version=2023-05-29`, {  // IBM API ì—”ë“œí¬ì¸íŠ¸ (ë²„ì „ì— ë§ê²Œ ì¡°ì •)
        method: 'POST',
        headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${API_KEY}`,
            'IBM-Project-ID': PROJECT_ID
        },
        body: JSON.stringify({
            model_id: 'ibm/granite-3-3-8b-instruct',
            input: prompt,
            parameters: {
                max_new_tokens: 200,
                temperature: 0.7
            }
        })
    });

    if (response.ok) {
        const data = await response.json();
        return data.results[0].generated_text;
    }
    throw new Error('LLM ì‹¤íŒ¨: ' + response.status);
}

// ====== TTS (í…ìŠ¤íŠ¸ â†’ ìŒì„±, IBM Watson) ======
async function textToSpeech(text) {
    const ttsVoice = 'ko-KR_JinV3Voice';
    const endpoint = `${TTS_URL}/v1/synthesize?voice=${ttsVoice}`;

    const response = await fetch(endpoint, {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json',
            'Accept': 'audio/webm'
        },
        body: JSON.stringify({ text }),
        // auth: fetchëŠ” Basic Auth ì‚¬ìš© (apikey:TTS_API_KEY)
        credentials: 'include'
    });

    if (response.ok) {
        const audioBlob = await response.blob();
        const audioUrl = URL.createObjectURL(audioBlob);
        ttsPlayer.src = audioUrl;
        ttsPlayer.play();
        statusDiv.textContent = 'ì‘ë‹µ ì¬ìƒ ì¤‘...';
    } else {
        throw new Error('TTS ì‹¤íŒ¨: ' + response.status);
    }
}

// ====== ìŒì„± ëª¨ë“œ ì¢…ë£Œ ======
function stopVoiceMode() {
    if (mediaRecorder) mediaRecorder.stop();
    if (stream) stream.getTracks().forEach(track => track.stop());
    if (audioContext) audioContext.close();
    ttsPlayer.pause();
}
