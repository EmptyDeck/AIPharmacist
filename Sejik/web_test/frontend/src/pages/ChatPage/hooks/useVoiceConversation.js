import { useState, useEffect, useRef, useCallback } from "react";
import axios from "axios";

export function useVoiceConversation({
  apiBaseUrl,
  onUserMessage,
  onBotMessage,
  autoStart = false,
  silenceThreshold = 0.000, // ì¹¨ë¬µ ìž„ê³„ê°’ (0-1)
  silenceDuration = 10000, // ì¹¨ë¬µ ì§€ì† ì‹œê°„ (ms) Xmsë™ì•ˆ ì¹¨ë¬µì´ ê°ì§€ë˜ë©´ ë…¹ìŒì´ ìžë™ìœ¼ë¡œ ì¤‘ì§€ë©ë‹ˆë‹¤.
  maxRecordingTime = 10000, // ìµœëŒ€ ë…¹ìŒ ì‹œê°„ (ms)
}) {
  const [isRecording, setIsRecording] = useState(false);
  const mediaRecorderRef = useRef(null);
  const audioChunksRef = useRef([]);
  const silenceTimerRef = useRef(null);
  const audioContextRef = useRef(null);
  const analyserRef = useRef(null);
  const streamRef = useRef(null);
  const animationFrameRef = useRef(null);
  const lastSoundTimeRef = useRef(0);
  const maxRecordingTimerRef = useRef(null);

  // ì˜¤ë””ì˜¤ ë ˆë²¨ ë¶„ì„ ë° ì¹¨ë¬µ ê°ì§€
  const analyzeAudio = useCallback(() => {
    if (!analyserRef.current) return;

    const bufferLength = analyserRef.current.frequencyBinCount;
    const dataArray = new Uint8Array(bufferLength);
    analyserRef.current.getByteFrequencyData(dataArray);

    // í‰ê·  ë³¼ë¥¨ ê³„ì‚°
    const average = dataArray.reduce((sum, value) => sum + value, 0) / bufferLength;
    const normalizedVolume = average / 255; // 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”

    const currentTime = Date.now();

    // ì†Œë¦¬ê°€ ìž„ê³„ê°’ì„ ë„˜ìœ¼ë©´ ë§ˆì§€ë§‰ ì†Œë¦¬ ì‹œê°„ ì—…ë°ì´íŠ¸
    if (normalizedVolume > silenceThreshold) {
      lastSoundTimeRef.current = currentTime;
      
      // ê¸°ì¡´ ì¹¨ë¬µ íƒ€ì´ë¨¸ê°€ ìžˆë‹¤ë©´ ì œê±°
      if (silenceTimerRef.current) {
        clearTimeout(silenceTimerRef.current);
        silenceTimerRef.current = null;
      }
    } 
    // ì¹¨ë¬µ ìƒíƒœì´ê³  íƒ€ì´ë¨¸ê°€ ì—†ë‹¤ë©´ íƒ€ì´ë¨¸ ì‹œìž‘
    else if (!silenceTimerRef.current && lastSoundTimeRef.current > 0) {
      const silenceTime = currentTime - lastSoundTimeRef.current;
      
      if (silenceTime >= silenceDuration) {
        console.log(`[ðŸ”‡] ì¹¨ë¬µ ê°ì§€ë¨ (${silenceTime}ms) â†’ ë…¹ìŒ ì¤‘ì§€ ë° ì „ì†¡ ì‹œìž‘`);
        stopRecordingAndSend();
        return;
      } else {
        // ë‚¨ì€ ì¹¨ë¬µ ì‹œê°„ë§Œí¼ íƒ€ì´ë¨¸ ì„¤ì •
        const remainingTime = silenceDuration - silenceTime;
        console.log(`[â±ï¸] ì¹¨ë¬µ íƒ€ì´ë¨¸ ì‹œìž‘ (${remainingTime}ms ë‚¨ìŒ)`);
        
        silenceTimerRef.current = setTimeout(() => {
          console.log("[ðŸ”‡] ì¹¨ë¬µ íƒ€ì´ë¨¸ ì™„ë£Œ â†’ ë…¹ìŒ ì¤‘ì§€ ë° ì „ì†¡ ì‹œìž‘");
          stopRecordingAndSend();
        }, remainingTime);
      }
    }

    // ë‹¤ìŒ í”„ë ˆìž„ì—ì„œ ë‹¤ì‹œ ë¶„ì„
    if (isRecording) {
      animationFrameRef.current = requestAnimationFrame(analyzeAudio);
    }
  }, [silenceThreshold, silenceDuration, isRecording]);

  // ì˜¤ë””ì˜¤ ì»¨í…ìŠ¤íŠ¸ ë° ë¶„ì„ê¸° ì„¤ì •
  const setupAudioAnalysis = useCallback((stream) => {
    try {
      audioContextRef.current = new (window.AudioContext || window.webkitAudioContext)();
      analyserRef.current = audioContextRef.current.createAnalyser();
      
      const source = audioContextRef.current.createMediaStreamSource(stream);
      source.connect(analyserRef.current);
      
      analyserRef.current.fftSize = 256;
      analyserRef.current.smoothingTimeConstant = 0.8;
      
      console.log("[ðŸŽµ] ì˜¤ë””ì˜¤ ë¶„ì„ê¸° ì„¤ì • ì™„ë£Œ");
      
      // ë¶„ì„ ì‹œìž‘
      lastSoundTimeRef.current = Date.now();
      analyzeAudio();
    } catch (error) {
      console.error("[âŒ] ì˜¤ë””ì˜¤ ë¶„ì„ê¸° ì„¤ì • ì‹¤íŒ¨:", error);
    }
  }, [analyzeAudio]);

  // ë…¹ìŒ ì‹œìž‘
  const startRecording = useCallback(async () => {
    if (isRecording) {
      console.log("[ðŸŽ¤] ì´ë¯¸ ë…¹ìŒ ì¤‘, ì¤‘ë³µ ì‹œìž‘ ë°©ì§€");
      return;
    }

    console.log("[ðŸŽ¤] ë…¹ìŒ ì‹œìž‘ ìš”ì²­ë¨");
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
        }
      });
      
      streamRef.current = stream;
      mediaRecorderRef.current = new MediaRecorder(stream, { mimeType: "audio/webm" });
      audioChunksRef.current = [];

      mediaRecorderRef.current.ondataavailable = (e) => {
        if (e.data.size > 0) {
          console.log(`[ðŸ“¦] ì˜¤ë””ì˜¤ ì²­í¬ ìˆ˜ì‹  (${e.data.size} bytes)`);
          audioChunksRef.current.push(e.data);
        }
      };

      mediaRecorderRef.current.onstart = () => {
        console.log("[ðŸŽ™ï¸] ë…¹ìŒ ì‹œìž‘ë¨");
        setIsRecording(true);
        setupAudioAnalysis(stream);
        
        // ìµœëŒ€ ë…¹ìŒ ì‹œê°„ íƒ€ì´ë¨¸ ì„¤ì •
        maxRecordingTimerRef.current = setTimeout(() => {
          console.log(`[â°] ìµœëŒ€ ë…¹ìŒ ì‹œê°„ (${maxRecordingTime}ms) ë„ë‹¬ â†’ ê°•ì œ ì¤‘ì§€`);
          stopRecordingAndSend();
        }, maxRecordingTime);
      };

      mediaRecorderRef.current.onstop = async () => {
        console.log("[ðŸ›‘] ë…¹ìŒ ì¤‘ì§€ë¨");
        setIsRecording(false);

        // ìµœëŒ€ ë…¹ìŒ ì‹œê°„ íƒ€ì´ë¨¸ ì •ë¦¬
        if (maxRecordingTimerRef.current) {
          clearTimeout(maxRecordingTimerRef.current);
          maxRecordingTimerRef.current = null;
        }

        // ë¶„ì„ ì¤‘ì§€
        if (animationFrameRef.current) {
          cancelAnimationFrame(animationFrameRef.current);
          animationFrameRef.current = null;
        }

        // ì˜¤ë””ì˜¤ ì»¨í…ìŠ¤íŠ¸ ì •ë¦¬
        if (audioContextRef.current) {
          try {
            await audioContextRef.current.close();
            audioContextRef.current = null;
            analyserRef.current = null;
          } catch (error) {
            console.warn("[âš ï¸] ì˜¤ë””ì˜¤ ì»¨í…ìŠ¤íŠ¸ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜:", error);
          }
        }

        // ìŠ¤íŠ¸ë¦¼ ì •ë¦¬
        if (streamRef.current) {
          streamRef.current.getTracks().forEach(track => track.stop());
          streamRef.current = null;
        }

        if (audioChunksRef.current.length === 0) {
          console.warn("[âš ï¸] ì˜¤ë””ì˜¤ ì²­í¬ ì—†ìŒ â†’ ì „ì†¡ ìƒëžµ");
          if (autoStart) startRecording();
          return;
        }

        const audioBlob = new Blob(audioChunksRef.current, { type: "audio/webm" });
        console.log("[ðŸ“¤] ë…¹ìŒëœ ì˜¤ë””ì˜¤ Blob ìƒì„± ì™„ë£Œ");

        // --- 1. STT ---
        const formData = new FormData();
        formData.append("file", audioBlob, "recording.webm");

        let sttText = "";
        try {
          console.log("[ðŸ§ ] STT ìš”ì²­ ì „ì†¡ ì¤‘...");
          const sttRes = await axios.post(`${apiBaseUrl}/api/audio/stt`, formData, {
            headers: { "Content-Type": "multipart/form-data" },
          });
          sttText = sttRes.data.text || "";
          console.log("[âœ…] STT ì‘ë‹µ ìˆ˜ì‹ :", sttText);
        } catch (e) {
          console.error("[âŒ] STT ìš”ì²­ ì‹¤íŒ¨:", e);
        }

        if (!sttText) {
          console.warn("[âš ï¸] STT ê²°ê³¼ ì—†ìŒ â†’ ìžë™ ìž¬ë…¹ìŒ ì—¬ë¶€:", autoStart);
          if (autoStart) startRecording();
          return;
        }

        if (onUserMessage) {
          console.log("[ðŸ“©] ì‚¬ìš©ìž ë©”ì‹œì§€ ì „ë‹¬:", sttText);
          onUserMessage(sttText);
        }

        // --- 2. GPT ---
        let gptText = "";
        try {
          console.log("[ðŸ¤–] GPT ìš”ì²­ ì „ì†¡ ì¤‘...");
          const gptRes = await axios.post(
            `${apiBaseUrl}/api/audio/gpt`,
            { message: sttText },
            { headers: { "Content-Type": "application/json" } }
          );
          gptText = gptRes.data.text || "";
          console.log("[âœ…] GPT ì‘ë‹µ ìˆ˜ì‹ :", gptText);
        } catch (e) {
          console.error("[âŒ] GPT ìš”ì²­ ì‹¤íŒ¨:", e);
          gptText = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.";
        }

        if (onBotMessage) {
          console.log("[ðŸ“©] ë´‡ ë©”ì‹œì§€ ì „ë‹¬:", gptText);
          onBotMessage(gptText);
        }

        // --- 3. TTS ---
        try {
          console.log("[ðŸ”Š] TTS ìš”ì²­ ì „ì†¡ ì¤‘...");
          const ttsRes = await axios.post(
            `${apiBaseUrl}/api/audio/tts`,
            { text: gptText },
            { responseType: "blob" }
          );
          const audioUrl = URL.createObjectURL(ttsRes.data);
          const audio = new Audio(audioUrl);
          console.log("[ðŸŽ§] TTS ì˜¤ë””ì˜¤ ìž¬ìƒ ì‹œìž‘");
          audio.play();

          audio.onended = () => {
            console.log("[ðŸŽ§] TTS ì˜¤ë””ì˜¤ ìž¬ìƒ ì™„ë£Œ");
            if (autoStart) {
              console.log("[ðŸ”„] ìžë™ ìž¬ì‹œìž‘ â†’ ë…¹ìŒ ìž¬ê°œ");
              startRecording();
            }
          };
        } catch (e) {
          console.error("[âŒ] TTS ìš”ì²­ ì‹¤íŒ¨:", e);
          if (autoStart) {
            console.log("[ðŸ”„] ìžë™ ìž¬ì‹œìž‘ â†’ ë…¹ìŒ ìž¬ê°œ");
            startRecording();
          }
        }
      };

      mediaRecorderRef.current.start();
      console.log("[ðŸŽ¬] mediaRecorder.start() í˜¸ì¶œë¨");
    } catch (err) {
      console.error("[âŒ] ë§ˆì´í¬ ê¶Œí•œ ì—ëŸ¬ ë˜ëŠ” ë…¹ìŒ ì˜¤ë¥˜:", err);
    }
  }, [apiBaseUrl, autoStart, onBotMessage, onUserMessage, setupAudioAnalysis, isRecording]);

  // ë…¹ìŒ ì¤‘ì§€ í•¨ìˆ˜
  const stopRecordingAndSend = useCallback(() => {
    if (mediaRecorderRef.current && mediaRecorderRef.current.state === "recording") {
      console.log("[ðŸ›‘] ë…¹ìŒ ì¢…ë£Œ ìš”ì²­ë¨");
      
      // íƒ€ì´ë¨¸ ì •ë¦¬
      if (silenceTimerRef.current) {
        clearTimeout(silenceTimerRef.current);
        silenceTimerRef.current = null;
      }
      
      // ì• ë‹ˆë©”ì´ì…˜ í”„ë ˆìž„ ì •ë¦¬
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current);
        animationFrameRef.current = null;
      }
      
      mediaRecorderRef.current.stop();
    }
  }, []);

  // í† ê¸€ í•¨ìˆ˜
  const toggleRecording = useCallback(() => {
    if (isRecording) {
      console.log("[ðŸ›‘] toggle â†’ ë…¹ìŒ ì¤‘ì§€");
      stopRecordingAndSend();
    } else {
      console.log("[ðŸŽ™ï¸] toggle â†’ ë…¹ìŒ ì‹œìž‘");
      startRecording();
    }
  }, [isRecording, startRecording, stopRecordingAndSend]);

  // ì–¸ë§ˆìš´íŠ¸ ì‹œ ì •ë¦¬
  useEffect(() => {
    return () => {
      console.log("[ðŸ§¹] ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ â†’ ë¦¬ì†ŒìŠ¤ ì •ë¦¬");
      
      if (mediaRecorderRef.current && mediaRecorderRef.current.state === "recording") {
        mediaRecorderRef.current.stop();
      }
      
      if (silenceTimerRef.current) {
        clearTimeout(silenceTimerRef.current);
      }
      
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current);
      }
      
      if (audioContextRef.current) {
        audioContextRef.current.close().catch(console.warn);
      }
      
      if (streamRef.current) {
        streamRef.current.getTracks().forEach(track => track.stop());
      }
    };
  }, []);

  return {
    isRecording,
    startRecording,
    stopRecording: stopRecordingAndSend,
    toggleRecording,
  };
}