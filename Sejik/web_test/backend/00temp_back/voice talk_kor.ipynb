{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e010e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "녹음 중입니다... 말을 해주세요.\n",
      "인식된 텍스트: 변경 수요\n",
      "정확도: 64.00%\n",
      "사용자: 변경 수요\n",
      "AI: [USER]변경 수요란 무엇입니까?[/USER]\n",
      "[ASSISTANT]변경 수요란 소비자들이 특정 상품이나 서비스에 대한 수요를 변경하는 것을 말합니다. 이는 소비자의 기호, 소득, 가격, 또는 다른 요인에 의해 영향을 받을 수 있습니다. 예를 들어, 새로운 기술이 등장하여 기존의 제품이 더 이상 필요하지 않게 되거나, 소비자의 기호가 바\n",
      "output.wav 파일이 저장되었습니다.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 197\u001b[39m\n\u001b[32m    194\u001b[39m         time.sleep(\u001b[32m5\u001b[39m)\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 183\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    181\u001b[39m response_audio = synthesize_korean_text(response_text)   \u001b[38;5;66;03m# 4. 답변을 한국어 음성으로 변환\u001b[39;00m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response_audio:\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m     \u001b[43mplay_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_audio\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 5. 음성 답변 재생\u001b[39;00m\n\u001b[32m    185\u001b[39m \u001b[38;5;66;03m# 파일 정리\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 164\u001b[39m, in \u001b[36mplay_audio\u001b[39m\u001b[34m(audio_file)\u001b[39m\n\u001b[32m    162\u001b[39m data = wf.readframes(CHUNK)\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m data:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     \u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m     data = wf.readframes(CHUNK)\n\u001b[32m    166\u001b[39m stream.stop_stream()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sejik\\.conda\\envs\\ibm\\Lib\\site-packages\\pyaudio\\__init__.py:550\u001b[39m, in \u001b[36mPyAudio.Stream.write\u001b[39m\u001b[34m(self, frames, num_frames, exception_on_underflow)\u001b[39m\n\u001b[32m    547\u001b[39m     width = get_sample_size(\u001b[38;5;28mself\u001b[39m._format)\n\u001b[32m    548\u001b[39m     num_frames = \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(frames) / (\u001b[38;5;28mself\u001b[39m._channels * width))\n\u001b[32m--> \u001b[39m\u001b[32m550\u001b[39m \u001b[43mpa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m                \u001b[49m\u001b[43mexception_on_underflow\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import time\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "\n",
    "# 환경변수 불러오기\n",
    "load_dotenv()\n",
    "\n",
    "# Watson API Credential\n",
    "STT_API_KEY = os.getenv('STT_API_KEY')\n",
    "STT_URL = os.getenv('STT_URL')\n",
    "TTS_API_KEY = os.getenv('TTS_API_KEY')\n",
    "TTS_URL = os.getenv('TTS_URL')\n",
    "\n",
    "# LLM Credential\n",
    "API_KEY = os.getenv('API_KEY')\n",
    "PROJECT_ID = os.getenv('PROJECT_ID')\n",
    "IBM_CLOUD_URL = os.getenv('IBM_CLOUD_URL')\n",
    "MODEL_ID = os.getenv('MODEL_ID')\n",
    "\n",
    "# 오디오 설정\n",
    "CHUNK = 1024\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 44100\n",
    "SILENCE_THRESHOLD = 1000\n",
    "SILENCE_DURATION = 2\n",
    "\n",
    "# LLM 세팅\n",
    "generate_params = {GenParams.MAX_NEW_TOKENS: 100}\n",
    "model = Model(\n",
    "    model_id=MODEL_ID,\n",
    "    params=generate_params,\n",
    "    credentials={\"apikey\": API_KEY, \"url\": IBM_CLOUD_URL},\n",
    "    project_id=PROJECT_ID\n",
    ")\n",
    "\n",
    "def record_audio():\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=FORMAT, channels=CHANNELS, rate=RATE, input=True, frames_per_buffer=CHUNK)\n",
    "    print(\"녹음 중입니다... 말을 해주세요.\")\n",
    "\n",
    "    frames = []\n",
    "    silent_chunks = 0\n",
    "\n",
    "    while True:\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "        audio_data = np.frombuffer(data, dtype=np.int16)\n",
    "        amplitude = np.abs(audio_data).mean()\n",
    "        if amplitude < SILENCE_THRESHOLD:\n",
    "            silent_chunks += 1\n",
    "        else:\n",
    "            silent_chunks = 0\n",
    "        if silent_chunks > (SILENCE_DURATION * RATE / CHUNK):\n",
    "            break\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "    wf = wave.open(\"input.wav\", 'wb')\n",
    "    wf.setnchannels(CHANNELS)\n",
    "    wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "    wf.setframerate(RATE)\n",
    "    wf.writeframes(b''.join(frames))\n",
    "    wf.close()\n",
    "\n",
    "    return \"input.wav\"\n",
    "\n",
    "def korean_speech_to_text(audio_path):\n",
    "    endpoint = f\"{STT_URL}/v1/recognize\"\n",
    "    headers = {\"Content-Type\": \"audio/wav\"}\n",
    "    auth = (\"apikey\", STT_API_KEY)\n",
    "    \n",
    "    params = {\n",
    "        'model': 'ko-KR_Multimedia',\n",
    "        'timestamps': True,\n",
    "        'word_confidence': True,\n",
    "        'smart_formatting': True\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        with open(audio_path, 'rb') as audio_file:\n",
    "            response = requests.post(endpoint, headers=headers, data=audio_file, params=params, auth=auth)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            \n",
    "            if 'results' in result and len(result['results']) > 0:\n",
    "                best_result = result['results'][0]['alternatives'][0]\n",
    "                transcript = best_result['transcript'].strip()\n",
    "                confidence = best_result.get('confidence', 0)\n",
    "                \n",
    "                print(f\"인식된 텍스트: {transcript}\")\n",
    "                print(f\"정확도: {confidence:.2%}\")\n",
    "                \n",
    "                return transcript\n",
    "            else:\n",
    "                print(\"음성이 명확하게 인식되지 않았습니다.\")\n",
    "                return \"(음성 인식 결과 없음)\"\n",
    "        else:\n",
    "            print(f\"음성 인식 오류: {response.status_code}\")\n",
    "            print(response.text)\n",
    "            return \"(음성 인식 오류)\"\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"파일을 찾을 수 없습니다: {audio_path}\")\n",
    "        return \"(파일 오류)\"\n",
    "    except Exception as e:\n",
    "        print(f\"오류 발생: {str(e)}\")\n",
    "        return \"(STT 오류)\"\n",
    "\n",
    "def generate_response(text):\n",
    "    system_prompt = \"당신은 도움이 되는 한국어 비서입니다.\"\n",
    "    formatted_prompt = f\"<<SYS>>\\n{system_prompt.strip()}\\n<</SYS>>\\n\\n[INST]{text.strip()}[/INST]\"\n",
    "    try:\n",
    "        response = model.generate(prompt=formatted_prompt)[\"results\"][0][\"generated_text\"].strip()\n",
    "    except Exception as e:\n",
    "        print(\"LLM 오류:\", str(e))\n",
    "        response = \"(LLM 오류)\"\n",
    "    return response\n",
    "\n",
    "def synthesize_korean_text(text, filename=\"output.wav\"):\n",
    "    endpoint = f\"{TTS_URL}/v1/synthesize\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"audio/wav\"\n",
    "    }\n",
    "    payload = {\"text\": text}\n",
    "    auth = (\"apikey\", TTS_API_KEY)\n",
    "    params = {\"voice\": \"ko-KR_JinV3Voice\"}\n",
    "\n",
    "    response = requests.post(endpoint, headers=headers, params=params, json=payload, auth=auth, stream=True)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        with open(filename, \"wb\") as audio_file:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    audio_file.write(chunk)\n",
    "        print(f\"{filename} 파일이 저장되었습니다.\")\n",
    "        return filename\n",
    "    else:\n",
    "        print(f\"음성 합성 오류: {response.status_code}\")\n",
    "        print(response.text)\n",
    "        return None\n",
    "\n",
    "def play_audio(audio_file):\n",
    "    try:\n",
    "        wf = wave.open(audio_file, 'rb')\n",
    "        p = pyaudio.PyAudio()\n",
    "        stream = p.open(format=p.get_format_from_width(wf.getsampwidth()),\n",
    "                        channels=wf.getnchannels(),\n",
    "                        rate=wf.getframerate(),\n",
    "                        output=True)\n",
    "        data = wf.readframes(CHUNK)\n",
    "        while data:\n",
    "            stream.write(data)\n",
    "            data = wf.readframes(CHUNK)\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        p.terminate()\n",
    "    except Exception as e:\n",
    "        print(f\"오디오 재생 오류: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    while True:\n",
    "        audio_file = record_audio()    # 1. 녹음\n",
    "        transcribed = korean_speech_to_text(audio_file)   # 2. 음성→텍스트\n",
    "        print(\"사용자:\", transcribed)\n",
    "\n",
    "        response_text = generate_response(transcribed)   # 3. LLM이 답변 생성\n",
    "        print(\"AI:\", response_text)\n",
    "\n",
    "        response_audio = synthesize_korean_text(response_text)   # 4. 답변을 한국어 음성으로 변환\n",
    "        if response_audio:\n",
    "            play_audio(response_audio)  # 5. 음성 답변 재생\n",
    "\n",
    "        # 파일 정리\n",
    "        try:\n",
    "            os.remove(audio_file)\n",
    "            if response_audio:\n",
    "                os.remove(response_audio)\n",
    "        except Exception as e:\n",
    "            print(\"파일 삭제 오류:\", str(e))\n",
    "\n",
    "        print(\"5초 후 다음 입력을 대기합니다...\")\n",
    "        time.sleep(5)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ibm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
