{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994eee89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording... Speak now.\n",
      "Stopped recording.\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# STT and TTS credentials\n",
    "STT_API_KEY = os.getenv('STT_API_KEY')\n",
    "STT_URL = os.getenv('STT_URL')\n",
    "TTS_API_KEY = os.getenv('TTS_API_KEY')\n",
    "TTS_URL = os.getenv('TTS_URL')\n",
    "\n",
    "# LLM credentials\n",
    "API_KEY = os.getenv('API_KEY')\n",
    "PROJECT_ID = os.getenv('PROJECT_ID')\n",
    "IBM_CLOUD_URL = os.getenv('IBM_CLOUD_URL')\n",
    "MODEL_ID = os.getenv('MODEL_ID')\n",
    "\n",
    "# Optional: Add error checking\n",
    "MISSING_ENV = []\n",
    "\n",
    "if not STT_API_KEY:\n",
    "    MISSING_ENV.append(\"STT_API_KEY\")\n",
    "if not STT_URL:\n",
    "    MISSING_ENV.append(\"STT_URL\")\n",
    "if not TTS_API_KEY:\n",
    "    MISSING_ENV.append(\"TTS_API_KEY\")\n",
    "if not TTS_URL:\n",
    "    MISSING_ENV.append(\"TTS_URL\")\n",
    "if not API_KEY:\n",
    "    MISSING_ENV.append(\"API_KEY\")\n",
    "if not PROJECT_ID:\n",
    "    MISSING_ENV.append(\"PROJECT_ID\")\n",
    "if not IBM_CLOUD_URL:\n",
    "    MISSING_ENV.append(\"IBM_CLOUD_URL\")\n",
    "if not MODEL_ID:\n",
    "    MISSING_ENV.append(\"MODEL_ID\")\n",
    "\n",
    "if MISSING_ENV:\n",
    "    raise ValueError(f\"Missing required environment variables: {', '.join(MISSING_ENV)}. Please check your .env file.\")\n",
    "\n",
    "\n",
    "# Audio recording settings\n",
    "CHUNK = 1024\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 44100\n",
    "SILENCE_THRESHOLD = 500  # Amplitude threshold for silence detection\n",
    "SILENCE_DURATION = 2  # Seconds of silence to stop recording\n",
    "\n",
    "# Initialize LLM\n",
    "generate_params = {GenParams.MAX_NEW_TOKENS: 900}\n",
    "model = Model(\n",
    "    model_id=MODEL_ID,\n",
    "    params=generate_params,\n",
    "    credentials={\"apikey\": API_KEY, \"url\": IBM_CLOUD_URL},\n",
    "    project_id=PROJECT_ID\n",
    ")\n",
    "\n",
    "def record_audio():\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=FORMAT, channels=CHANNELS, rate=RATE, input=True, frames_per_buffer=CHUNK)\n",
    "    print(\"Recording... Speak now.\")\n",
    "    \n",
    "    frames = []\n",
    "    silent_chunks = 0\n",
    "    recording = True\n",
    "    \n",
    "    while recording:\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "        \n",
    "        # Convert audio chunk to numpy array for amplitude analysis\n",
    "        audio_data = np.frombuffer(data, dtype=np.int16)\n",
    "        amplitude = np.abs(audio_data).mean()\n",
    "        \n",
    "        # Check for silence\n",
    "        if amplitude < SILENCE_THRESHOLD:\n",
    "            silent_chunks += 1\n",
    "        else:\n",
    "            silent_chunks = 0\n",
    "        \n",
    "        # Stop recording after sustained silence\n",
    "        if silent_chunks > (SILENCE_DURATION * RATE / CHUNK):\n",
    "            recording = False\n",
    "    \n",
    "    print(\"Stopped recording.\")\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "    \n",
    "    # Save audio to WAV file\n",
    "    wf = wave.open(\"input.wav\", 'wb')\n",
    "    wf.setnchannels(CHANNELS)\n",
    "    wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "    wf.setframerate(RATE)\n",
    "    wf.writeframes(b''.join(frames))\n",
    "    wf.close()\n",
    "    \n",
    "    return \"input.wav\"\n",
    "\n",
    "def speech_to_text(audio_file):\n",
    "    headers = {\"Content-Type\": \"audio/wav\"}\n",
    "    with open(audio_file, 'rb') as f:\n",
    "        response = requests.post(\n",
    "            STT_URL,\n",
    "            headers=headers,\n",
    "            data=f,\n",
    "            auth=(\"apikey\", STT_API_KEY)\n",
    "        )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        if result.get(\"results\"):\n",
    "            return result[\"results\"][0][\"alternatives\"][0][\"transcript\"]\n",
    "    return \"Error transcribing audio.\"\n",
    "\n",
    "def generate_response(text):\n",
    "    system_prompt = \"You are a helpful assistant.\"\n",
    "    formatted_prompt = f\"<<SYS>>\\n{system_prompt.strip()}\\n<</SYS>>\\n\\n[INST]{text.strip()}[/INST]\"\n",
    "    response = model.generate(prompt=formatted_prompt)[\"results\"][0][\"generated_text\"].strip()\n",
    "    return response\n",
    "\n",
    "def text_to_speech(text):\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"audio/wav\"\n",
    "    }\n",
    "    data = json.dumps({\"text\": text})\n",
    "    response = requests.post(\n",
    "        TTS_URL + \"?voice=en-US_MichaelV3Voice\",\n",
    "        headers=headers,\n",
    "        data=data,\n",
    "        auth=(\"apikey\", TTS_API_KEY)\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        with open(\"output.wav\", \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        return \"output.wav\"\n",
    "    return None\n",
    "\n",
    "def play_audio(audio_file):\n",
    "    wf = wave.open(audio_file, 'rb')\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(\n",
    "        format=p.get_format_from_width(wf.getsampwidth()),\n",
    "        channels=wf.getnchannels(),\n",
    "        rate=wf.getframerate(),\n",
    "        output=True\n",
    "    )\n",
    "    \n",
    "    data = wf.readframes(CHUNK)\n",
    "    while data:\n",
    "        stream.write(data)\n",
    "        data = wf.readframes(CHUNK)\n",
    "    \n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "def main():\n",
    "    while True:\n",
    "        # Step 1: Record audio\n",
    "        audio_file = record_audio()\n",
    "        \n",
    "        # Step 2: Transcribe audio to text\n",
    "        transcribed_text = speech_to_text(audio_file)\n",
    "        print(f\"User said: {transcribed_text}\")\n",
    "        \n",
    "        if \"Error\" in transcribed_text:\n",
    "            print(\"Failed to transcribe speech. Try again.\")\n",
    "            continue\n",
    "        \n",
    "        # Step 3: Generate response with LLM\n",
    "        response_text = generate_response(transcribed_text)\n",
    "        print(f\"AI response: {response_text}\")\n",
    "        \n",
    "        # Step 4: Convert response to speech\n",
    "        response_audio = text_to_speech(response_text)\n",
    "        if response_audio:\n",
    "            # Step 5: Play the response\n",
    "            play_audio(response_audio)\n",
    "        \n",
    "        # Clean up\n",
    "        os.remove(audio_file)\n",
    "        if response_audio:\n",
    "            os.remove(response_audio)\n",
    "        print(\"Puase for 5 seconds\")\n",
    "        time.sleep(5)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ibm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
